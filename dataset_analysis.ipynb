{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "skilled-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from twokenize import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "postal-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distinct-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      "Unnamed: 0            24783 non-null int64\n",
      "count                 24783 non-null int64\n",
      "hate_speech           24783 non-null int64\n",
      "offensive_language    24783 non-null int64\n",
      "neither               24783 non-null int64\n",
      "class                 24783 non-null int64\n",
      "tweet                 24783 non-null object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-devil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# count: (Integer) number of users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable,\n",
    "# hate_speech_annotation: (Integer) number of users who judged the tweet to be hate speech,\n",
    "# offensive_language_annotation: (Integer) number of users who judged the tweet to be offensive,\n",
    "# neither_annotation: (Integer) number of users who judged the tweet to be neither offensive nor non-offensive,\n",
    "# label: (Class Label) class label for majority of CF users (0: 'hate-speech', 1: 'offensive-language' or 2: 'neither'),\n",
    "# tweet: (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "horizontal-silicon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0     1430\n",
       "1    19190\n",
       "2     4163\n",
       "Name: tweet, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label distribution\n",
    "data.groupby('class').count()['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handled-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_hate = data[(data['offensive_language'] == 0) & (data['neither'] == 0)]\n",
    "pure_offensive = data[(data['hate_speech'] == 0) & (data['neither'] == 0)]\n",
    "pure_neither = data[(data['offensive_language'] == 0) & (data['hate_speech'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "minimal-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "14347\n",
      "2872\n"
     ]
    }
   ],
   "source": [
    "print(len(pure_hate))\n",
    "print(len(pure_offensive))\n",
    "print(len(pure_neither))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grateful-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "pure_hate['token'] = pure_hate['tweet'].apply(func=tokenize)\n",
    "pure_offensive['token'] = pure_offensive['tweet'].apply(func=tokenize)\n",
    "pure_neither['token'] = pure_neither['tweet'].apply(func=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "occupied-bridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@CB_Baby24: @white_thunduh alsarabsss\" hes a ...</td>\n",
       "      <td>[\", @CB_Baby24, :, @white_thunduh, alsarabsss,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@DevilGrimz: @VigxRArts you're fucking gay, b...</td>\n",
       "      <td>[\", @DevilGrimz, :, @VigxRArts, you're, fuckin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>186</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"@MarkRoundtreeJr: LMFAOOOO I HATE BLACK PEOPL...</td>\n",
       "      <td>[\", @MarkRoundtreeJr, :, LMFAOOOO, I, HATE, BL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>466</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Hey go look at that video of the man that fou...</td>\n",
       "      <td>[\", Hey, go, look, at, that, video, of, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>549</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Our people\". Now is the time for the Aryan ra...</td>\n",
       "      <td>[\", Our, people, \", ., Now, is, the, time, for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "89           90      3            3                   0        0      0   \n",
       "110         111      3            3                   0        0      0   \n",
       "184         186      3            3                   0        0      0   \n",
       "459         466      3            3                   0        0      0   \n",
       "540         549      3            3                   0        0      0   \n",
       "\n",
       "                                                 tweet  \\\n",
       "89   \"@CB_Baby24: @white_thunduh alsarabsss\" hes a ...   \n",
       "110  \"@DevilGrimz: @VigxRArts you're fucking gay, b...   \n",
       "184  \"@MarkRoundtreeJr: LMFAOOOO I HATE BLACK PEOPL...   \n",
       "459  \"Hey go look at that video of the man that fou...   \n",
       "540  \"Our people\". Now is the time for the Aryan ra...   \n",
       "\n",
       "                                                 token  \n",
       "89   [\", @CB_Baby24, :, @white_thunduh, alsarabsss,...  \n",
       "110  [\", @DevilGrimz, :, @VigxRArts, you're, fuckin...  \n",
       "184  [\", @MarkRoundtreeJr, :, LMFAOOOO, I, HATE, BL...  \n",
       "459  [\", Hey, go, look, at, that, video, of, the, m...  \n",
       "540  [\", Our, people, \", ., Now, is, the, time, for...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_hate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "balanced-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_tokens = {i: row['token'] for i, row in pure_hate.iterrows()}\n",
    "poffensive_tokens = {i: row['token'] for i, row in pure_offensive.iterrows()}\n",
    "pneither_tokens = {i: row['token'] for i, row in pure_neither.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "processed-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sufficient-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wf(corpus_lst):\n",
    "    bow = CountVectorizer(min_df=2, stop_words='english', tokenizer=tokenize)\n",
    "    cv = bow.fit_transform(corpus_lst)\n",
    "    tmp_sum = cv.sum(axis=0)\n",
    "    wf = [(word, tmp_sum[0, idx]) for word, idx in bow.vocabulary_.items()]\n",
    "    wf = sorted(wf, key=lambda x: x[1], reverse=True)\n",
    "    return wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "intellectual-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tks = ['.', ',', ':', ';', '\"', '&', 'rt']\n",
    "wf_phate = get_wf(pure_hate['tweet'].tolist())\n",
    "wf_phate = list(filter(lambda x: x[0] not in stop_tks ,wf_phate))\n",
    "wf_poffensive = get_wf(pure_offensive['tweet'].tolist())\n",
    "wf_poffensive = list(filter(lambda x: x[0] not in stop_tks ,wf_poffensive))\n",
    "wf_pneither = get_wf(pure_neither['tweet'].tolist())\n",
    "wf_pneither = list(filter(lambda x: x[0] not in stop_tks ,wf_pneither))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "expensive-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitch', 6545),\n",
       " ('bitches', 2516),\n",
       " ('#128514', 2294),\n",
       " ('like', 1759),\n",
       " ('hoes', 1742),\n",
       " ('pussy', 1715),\n",
       " ('hoe', 1356),\n",
       " ('#8220', 1137),\n",
       " (\"i'm\", 1072),\n",
       " ('?', 1065),\n",
       " ('ass', 1017),\n",
       " ('!', 1008),\n",
       " ('fuck', 970),\n",
       " ('shit', 937),\n",
       " (\"don't\", 926),\n",
       " ('#8221', 902),\n",
       " ('got', 870),\n",
       " ('just', 840),\n",
       " ('...', 796),\n",
       " ('u', 775)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 20\n",
    "# wf_phate[:top_n]\n",
    "wf_poffensive[:top_n]\n",
    "# wf_pneither[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hired-conversation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trash', 383),\n",
       " ('!', 380),\n",
       " ('?', 276),\n",
       " ('bird', 239),\n",
       " ('#8230', 233),\n",
       " ('charlie', 220),\n",
       " ('like', 198),\n",
       " ('...', 185),\n",
       " ('just', 176),\n",
       " ('yellow', 171),\n",
       " ('birds', 148),\n",
       " ('#9733', 140),\n",
       " (\"i'm\", 126),\n",
       " (\"it's\", 121),\n",
       " ('amp', 118),\n",
       " ('#8220', 116),\n",
       " ('yankees', 114),\n",
       " ('-', 103),\n",
       " ('..', 101),\n",
       " ('#128514', 99)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_pneither[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "alpha-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_feature(df_train, df_test, *args):\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=args[0], tokenizer=tokenize, binary=args[1], min_df=2, max_df=512, stop_words='english')\n",
    "\n",
    "    tr_x = vectorizer.fit_transform(df_train['tweet']).toarray()\n",
    "    ts_x = vectorizer.transform(df_test['tweet']).toarray()\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(\"Train: \", tr_x.shape)\n",
    "    print(\"Test: \", ts_x.shape)\n",
    "    return tr_x, ts_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "express-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_y, test_y = train['class'].to_numpy(), test['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "sought-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!!', '!!!', '\" &', '#128530', '#128530 ;', '#128553', '#128553 ;', '#128557', '#128557 ;', '#65039', '#65039 ;', '#8217', '& #128530', '& #128553', '& #128557', '& #65039', '& #8230', \"'\", '*', ', bitch', '-', '. \"', '. &', '....', '2', ': \"', ': &', ': bitch', 'aint', 'bad', 'best', 'better', 'big', 'bird', 'birds', 'bitch &', 'bitch ,', 'bitch ass', 'black', 'bout', 'boy', \"can't\", 'cause', 'charlie', 'come', 'cunt', 'cuz', 'da', 'damn', 'dat', 'day', 'dick', 'did', 'dont', 'dumb', 'eat', 'fag', 'faggot', 'feel', 'fuckin', 'fucking', 'game', 'getting', 'ghetto', 'girl', 'girls', 'going', 'gonna', 'good', 'gotta', 'hate', 'hit', \"i'll\", 'im', \"it's\", 'let', 'life', 'like bitch', 'lil', 'little', 'lmao', 'look', 'look like', 'mad', 'make', 'man', 'money', 'n', 'need', 'new', 'niggah', 'nigger', 'old', 'people', 'play', 'real', 'really', 'retarded', 'right', 'said', 'say', 'stop', 'stupid', 'talk', 'talking', 'tell', \"that's\", 'think', 'tho', 'time', 'today', 'trash .', 'twitter', 'ugly', 'ur', 'wanna', 'want', 'way', 'white', 'wit', \"y'all\", 'ya', 'yall', 'yeah', 'yellow', 'yo', 'yo bitch', \"you're\"]\n",
      "Train:  (19826, 128)\n",
      "Test:  (4957, 128)\n"
     ]
    }
   ],
   "source": [
    "# vectorizer will be used for feature selection later on\n",
    "train_x, test_x = encode_feature(train, test, 128, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "reasonable-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "pleased-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 24783\n",
    "classifier = svm.SVC(gamma=0.1, class_weight={0: N/1340, 1: N/19190, 2: N/4163}, C=1.0)\n",
    "classifier.fit(train_x, train_y)\n",
    "preds = classifier.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "burning-alliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.135     0.327     0.191       300\n",
      "           1      0.922     0.339     0.495      3852\n",
      "           2      0.242     0.846     0.376       805\n",
      "\n",
      "    accuracy                          0.420      4957\n",
      "   macro avg      0.433     0.504     0.354      4957\n",
      "weighted avg      0.764     0.420     0.458      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, preds, digits=3, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dramatic-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "republican-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['! \"', '! &', '!!', '!!!', '\" \"', '\" &', '\" -', '\" .', '\" bitch', '\" i\\'m', '#12288', '#12288 ;', '#128064', '#128064 ;', '#128074', '#128074 ;', '#128076', '#128076 ;', '#128079', '#128079 ;', '#128109', '#128109 ;', '#128128', '#128128 ;', '#128129', '#128129 ;', '#128175', '#128175 ;', '#128515', '#128515 ;', '#128520', '#128520 ;', '#128525', '#128525 ;', '#128526', '#128526 ;', '#128527', '#128527 ;', '#128530', '#128530 ;', '#128536', '#128536 ;', '#128553', '#128553 ;', '#128557', '#128557 ;', '#128563', '#128563 ;', '#128564', '#128564 ;', '#128588', '#128588 ;', '#65039', '#65039 ;', '#8217', '#8217 ;', '#8217 ;s', '#9733', '#9733 ;', '#9995', '#9995 ;', '#9996', '#9996 ;', '#yankees', '& #12288', '& #128064', '& #128074', '& #128076', '& #128079', '& #128109', '& #128128', '& #128129', '& #128175', '& #128515', '& #128525', '& #128526', '& #128527', '& #128530', '& #128536', '& #128553', '& #128557', '& #128563', '& #128564', '& #128588', '& #65039', '& #8230', '& #9733', '& #9995', '& #9996', '& gt', '& lt', '&gt;', \"'\", '(', ')', '*', ', &', ', bitch', ', bitches', \", don't\", \", i'm\", ', just', '-', '. \"', '. &', '. .', '. bitch', \". don't\", \". i'm\", \". it's\", '. just', '. lol', '. rt', '....', '.....', '1', '10', '2', '3', '4', '5', ': \"', ': &', ': bitch', ': bitches', \": don't\", ': fuck', ': hate', ': hoes', \": i'm\", ': just', ': niggas', ': rt', '; \"', '; .', '; bitch', '; rt', '; t', ';s', '=', '? \"', '? &', '@', '@kieffer_jason', 'act', 'actually', 'af', 'aint', 'ask', 'ass bitch', 'ass nigga', 'away', 'b', 'baby', 'bad', 'bad bitch', 'bad bitches', 'bae', 'basic', 'bc', 'beat', 'believe', 'best', 'bet', 'better', 'big', 'bird', 'birds', 'birthday', 'bitch !', 'bitch \"', 'bitch &', 'bitch ,', 'bitch ?', 'bitch ass', \"bitch don't\", 'bitch got', 'bitch nigga', 'bitch u', 'bitch&', 'bitches &', 'bitches ,', 'bitches .', 'black', 'bout', 'boy', 'boys', 'bring', 'bro', 'broke', 'brownies', 'bruh', 'buy', 'called', 'calling', \"can't\", 'car', 'care', 'catch', 'cause', 'charlie', 'colored', 'come', 'cool', \"couldn't\", 'crazy', 'cunt', 'cut', 'cute', 'cuz', 'da', 'damn', 'dat', 'day', 'days', 'dead', 'dem', 'dick', 'did', \"didn't\", 'die', 'dis', 'does', \"doesn't\", 'dog', 'doing', \"don't know\", \"don't like\", \"don't want\", 'dont', 'dude', 'dumb', 'dumb bitch', 'dyke', 'eat', 'eat pussy', 'eating', 'em', 'everybody', 'face', 'fag', 'faggot', 'faggots', 'fags', 'fake', 'fat', 'feel', 'females', 'fight', 'follow', 'food', 'free', 'friend', 'friends', 'fuck .', 'fuck bitch', 'fucked', 'fuckin', 'fucking', 'fun', 'funny', 'game', 'gave', 'gay', 'gets', 'getting', 'ghetto', 'girl', 'girls', 'god', 'going', 'gon', 'gone', 'gonna', 'good', 'got hoes', 'gotta', 'great', 'gt', 'gt ;', 'guess', 'guy', 'guys', 'haha', 'hair', 'half', 'happy', 'hard', 'hate', 'having', \"he's\", 'head', 'hell', 'hey', 'high', 'hit', 'ho', 'hoe &', 'hoe ,', 'hoe .', 'hoes &', 'hoes ,', 'hoes .', \"hoes ain't\", 'home', 'hope', 'hot', 'house', \"i'd\", \"i'll\", \"i've\", 'ill', 'im', 'ima', 'ion', \"isn't\", 'job', 'jus', 'kids', 'kill', 'leave', 'left', 'let', 'life', 'like \"', 'like bitch', 'lil', 'lil bitch', 'little', 'little bitch', 'live', 'lmao', 'lmfao', 'long', 'look', 'look like', 'looking', 'looks', 'lost', 'lot', 'love', 'loyal', 'lt', 'lt ;', 'mad', 'main', 'make', 'makes', 'making', 'man', 'maybe', 'mean', 'men', 'mind', 'mock', 'mom', 'money', 'monkey', 'mouth', 'music', 'n', 'nah', 'need', 'needs', 'new', 'nicca', 'niccas', 'nice', 'nig', 'niggah', 'nigger', 'niggers', 'nigguh', 'night', 'oh', 'ok', 'okay', 'old', 'omg', 'outta', 'party', 'pay', 'people', 'person', 'phone', 'pic', 'play', 'playing', 'pretty', 'probably', 'pull', 'pussies', 'pussy .', 'queer', 'r', 'real', 'really', 'red', 'redneck', 'remember', 'retard', 'retarded', 'right', 'run', 'said', 'said \"', 'saw', 'say', 'saying', 'says', 'school', 'seen', 'sex', \"she's\", 'shit .', 'shut', 'single', 'sleep', 'smh', 'smoke', 'son', 'song', 'sorry', 'stand', 'start', 'stay', 'stfu', 'stop', 'straight', 'stupid', 'suck', 'sure', 'swear', 't', 'talk', 'talking', 'team', 'tell', 'text', 'tf', 'thank', 'thanks', \"that's\", 'thats', \"there's\", \"they're\", 'thing', 'things', 'think', 'tho', 'thought', 'throw', 'time', 'today', 'told', 'tonight', 'took', 'trash .', 'trust', 'try', 'trying', 'tryna', 'turn', 'tweet', 'tweets', 'twitter', 'type', 'ugly', 'ur', 'use', 'used', 'wait', 'wanna', 'want', 'wanted', 'wants', 'watch', 'watching', 'way', 'wear', 'weed', 'went', \"what's\", 'white', 'white trash', 'wife', 'win', 'wish', 'wit', 'woman', 'women', \"won't\", 'word', 'work', 'world', 'wrong', 'wtf', \"y'all\", 'ya', 'yall', 'yankees', 'yea', 'yeah', 'year', 'years', 'yellow', 'yes', 'yo', 'yo bitch', \"you're\", 'young', '|']\n",
      "Train:  (19826, 512)\n",
      "Test:  (4957, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x = encode_feature(train, test, 512, False)\n",
    "counter = {}\n",
    "mi = mutual_info_classif(train_x, train_y, n_neighbors=8, discrete_features=True, random_state=424)\n",
    "train_x\n",
    "# vec.get_feature_names()\n",
    "# for i in mi.argsort()[::-1]:\n",
    "#     counter[vec.get_feature_names()[i]] = counter.get(vec.get_feature_names()[i], 0) + 1\n",
    "# print(sorted(counter.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "proper-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 128)\n"
     ]
    }
   ],
   "source": [
    "features = SelectKBest(mutual_info_classif, k=128)\n",
    "train_x = features.fit_transform(train_x, train_y)\n",
    "# test_x = features.transform(test_x)\n",
    "test_x = test_x[:, features.get_support(indices=True)]\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "stainless-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(gamma=0.1, class_weight={0: N/1340, 1: N/19190, 2: N/4163}, C=1.0)\n",
    "classifier.fit(train_x, train_y)\n",
    "preds = classifier.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "american-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.280     0.500     0.359       284\n",
      "           1      0.846     0.862     0.854      3895\n",
      "           2      0.550     0.339     0.420       778\n",
      "\n",
      "    accuracy                          0.760      4957\n",
      "   macro avg      0.559     0.567     0.544      4957\n",
      "weighted avg      0.767     0.760     0.758      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, preds, digits=3, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "homeless-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "serial-industry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['! \"', '! &', '!!', '!!!', '\" \"', '\" &', '\" -', '\" .', '\" bitch', '\" i\\'m', '#1041204', '#1041204 ;', '#12288', '#12288 ;', '#128064', '#128064 ;', '#128074', '#128074 ;', '#128076', '#128076 ;', '#128079', '#128079 ;', '#128128', '#128128 ;', '#128129', '#128129 ;', '#128175', '#128175 ;', '#128515', '#128515 ;', '#128525', '#128525 ;', '#128526', '#128526 ;', '#128527', '#128527 ;', '#128530', '#128530 ;', '#128536', '#128536 ;', '#128553', '#128553 ;', '#128557', '#128557 ;', '#128563', '#128563 ;', '#128564', '#128564 ;', '#128588', '#128588 ;', '#65039', '#65039 ;', '#8217', '#8217 ;', '#8217 ;s', '#9733', '#9733 ;', '#9995', '#9995 ;', '#9996', '#9996 ;', '#yankees', '& #1041204', '& #12288', '& #128064', '& #128074', '& #128076', '& #128079', '& #128128', '& #128129', '& #128175', '& #128515', '& #128525', '& #128527', '& #128530', '& #128536', '& #128553', '& #128557', '& #128563', '& #128564', '& #128588', '& #65039', '& #8230', '& #9733', '& #9995', '& #9996', '& gt', '& lt', '&gt;', \"'\", '(', ')', '*', ', \"', ', &', ', bitch', ', bitches', \", don't\", \", i'm\", ', just', '-', '. \"', '. &', '. .', \". don't\", \". i'm\", \". it's\", '. just', '. lol', '. rt', '....', '.....', '1', '2', '3', '4', '5', ': \"', ': &', ': bitch', ': bitches', \": don't\", ': hate', ': hoes', \": i'm\", ': just', ': niggas', ': rt', ': u', '; \"', '; .', '; @&', '; bitch', '; rt', '; t', ';s', '=', '? \"', '? &', '??', '@', '@&', '@kieffer_jason', 'act', 'act like', 'af', 'aint', 'ask', 'ass bitch', 'ass nigga', 'away', 'b', 'baby', 'bad', 'bad bitch', 'bad bitches', 'bae', 'basic', 'bc', 'beat', 'believe', 'best', 'bet', 'better', 'big', 'bird', 'birds', 'birthday', 'bitch !', 'bitch \"', 'bitch &', 'bitch ,', 'bitch ?', 'bitch ass', \"bitch don't\", 'bitch got', \"bitch i'm\", 'bitch nigga', 'bitch&', 'bitches &', 'bitches ,', 'bitches .', 'bitches like', 'black', 'bout', 'boy', 'bring', 'bro', 'broke', 'brownies', 'bruh', 'buy', 'called', 'calling', 'came', \"can't\", 'car', 'care', 'cause', 'charlie', 'class', 'club', 'colored', 'come', 'cool', \"couldn't\", 'crazy', 'cunt', 'cut', 'cute', 'cuz', 'da', 'damn', 'dat', 'day', 'days', 'dem', 'dick', 'did', \"didn't\", 'die', 'dis', 'does', \"doesn't\", 'dog', 'doing', \"don't know\", \"don't like\", 'dont', 'dude', 'dumb', 'dumb bitch', 'eat', 'eat pussy', 'eating', 'em', 'everybody', 'face', 'fag', 'faggot', 'faggots', 'fags', 'fake', 'fat', 'feel', 'feel like', 'females', 'fight', 'follow', 'food', 'free', 'friend', 'friends', 'fuck .', 'fuck bitch', 'fucked', 'fuckin', 'fucking', 'funny', 'game', 'gave', 'gay', 'gets', 'getting', 'ghetto', 'girl', 'girls', 'god', 'goes', 'going', 'gon', 'gone', 'gonna', 'good', 'got hoes', 'gotta', 'great', 'gt', 'gt ;', 'guess', 'guy', 'guys', 'haha', 'hair', 'half', 'happy', 'hard', 'hate', 'having', \"he's\", 'head', 'hell', 'hey', 'high', 'hit', 'ho', 'hoe &', 'hoe ,', 'hoe .', 'hoes &', 'hoes ,', 'hoes .', \"hoes ain't\", 'home', 'hope', 'hot', 'house', \"i'd\", \"i'll\", \"i've\", 'idk', 'im', 'ima', 'ion', \"isn't\", 'job', 'jus', 'kids', 'kill', 'leave', 'left', 'let', 'life', 'like \"', 'like bitch', 'lil', 'lil bitch', 'line', 'little', 'little bitch', 'live', 'lmao', 'lmfao', 'long', 'look', 'look like', 'looking', 'looks', 'lost', 'lot', 'loyal', 'lt', 'lt ;', 'mad', 'main', 'make', 'makes', 'making', 'man', 'maybe', 'mean', 'men', 'mind', 'miss', 'mom', 'money', 'monkey', 'morning', 'mouth', 'music', 'n', 'nah', 'nasty', 'need', 'needs', 'new', 'nicca', 'niccas', 'nice', 'nig', 'niggah', 'nigger', 'niggers', 'nigguh', 'night', 'oh', 'ok', 'old', 'omg', 'outta', 'party', 'pay', 'people', 'person', 'phone', 'pic', 'play', 'playing', 'pretty', 'probably', 'pull', 'pussies', 'pussy .', 'queer', 'r', 'real', 'really', 'red', 'redneck', 'remember', 'retard', 'retarded', 'right', 'run', 'said', 'said \"', 'saw', 'say', 'saying', 'says', 'school', 'seen', 'sex', \"she's\", 'shit .', 'shut', 'single', 'sleep', 'smh', 'somebody', 'son', 'song', 'sorry', 'start', 'stay', 'stfu', 'stop', 'straight', 'stupid', 'suck', 'sure', 'swear', 't', 'talk', 'talking', 'team', 'tell', 'text', 'tf', 'thanks', \"that's\", 'thats', \"there's\", \"they're\", 'thing', 'things', 'think', 'tho', 'thought', 'throw', 'time', 'today', 'told', 'tonight', 'took', 'trash .', 'true', 'trust', 'try', 'trying', 'tryna', 'turn', 'tweet', 'tweets', 'twitter', 'type', 'ugly', 'ur', 'use', 'used', 'wait', 'wanna', 'want', 'wanted', 'wants', 'watch', 'watching', 'way', 'wear', 'weed', 'went', \"what's\", 'white', 'white trash', 'wife', 'win', 'wish', 'wit', 'woman', 'women', \"won't\", 'word', 'work', 'world', 'wow', 'wrong', 'wtf', \"y'all\", 'ya', 'yall', 'yankees', 'yea', 'yeah', 'year', 'years', 'yellow', 'yes', 'yo', 'yo bitch', \"you're\", 'young', '|']\n",
      "Train:  (19826, 512)\n",
      "Test:  (4957, 512)\n"
     ]
    }
   ],
   "source": [
    "# encode tf-idf as features\n",
    "train_x, test_x = encode_feature(train, test, 512, False)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "train_x = tfidf_transformer.fit_transform(train_x)\n",
    "test_x = tfidf_transformer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "exclusive-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC(gamma=0.1, class_weight={0: N/1340, 1: N/19190, 2: N/4163}, C=1.0)\n",
    "classifier.fit(train_x, train_y)\n",
    "preds = classifier.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "every-spencer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.248     0.597     0.351       320\n",
      "           1      0.919     0.639     0.754      3817\n",
      "           2      0.410     0.768     0.535       820\n",
      "\n",
      "    accuracy                          0.657      4957\n",
      "   macro avg      0.526     0.668     0.546      4957\n",
      "weighted avg      0.792     0.657     0.692      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, preds, digits=3, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "round-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19826, 128)\n"
     ]
    }
   ],
   "source": [
    "features = SelectKBest(mutual_info_classif, k=128)\n",
    "train_x = features.fit_transform(train_x, train_y)\n",
    "# test_x = features.transform(test_x)\n",
    "test_x = test_x[:, features.get_support(indices=True)]\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "minus-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.252     0.475     0.329       320\n",
      "           1      0.856     0.833     0.845      3817\n",
      "           2      0.629     0.489     0.550       820\n",
      "\n",
      "    accuracy                          0.753      4957\n",
      "   macro avg      0.579     0.599     0.575      4957\n",
      "weighted avg      0.779     0.753     0.763      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(gamma=0.1, class_weight={0: N/1340, 1: N/19190, 2: N/4163}, C=1.0)\n",
    "classifier.fit(train_x, train_y)\n",
    "preds = classifier.predict(test_x)\n",
    "print(metrics.classification_report(test_y, preds, digits=3, output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "integrated-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a feature heat map showing the correlation of features\n",
    "import seaborn as sns\n",
    "\n",
    "train_x_df = pd.DataFrame(train_x)\n",
    "train_x_df.head()\n",
    "sns.heatmap(train_x_df.corr(), annot= True, fmt= '.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selective-bridal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fde113f6c90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "seed = 424\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aggregate-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "def encode_fn(text_list):\n",
    "    all_input_ids = []    \n",
    "    for text in text_list:\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "        all_input_ids.append(input_ids)    \n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    return all_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sorted-profile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_x = encode_fn(train['tweet'])\n",
    "train_y = torch.tensor(train['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = encode_fn(test['tweet'])\n",
    "test_y = torch.tensor(test['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "living-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "official-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "russian-legislation",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2f9f6f52274c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    total_eval_accuracy = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.zero_grad()\n",
    "        loss, logits = model(batch[0], token_type_ids=None, attention_mask=(batch[0]>0), labels=batch[1])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step() \n",
    "        scheduler.step()\n",
    "        \n",
    "    model.eval()\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        with torch.no_grad():\n",
    "            loss, logits = model(batch[0], token_type_ids=None, attention_mask=(batch[0]>0), labels=batch[1])\n",
    "                \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch[1].to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    \n",
    "    print(f'Train loss     : {avg_train_loss}')\n",
    "    print(f'Validation loss: {avg_val_loss}')\n",
    "    print(f'Accuracy: {avg_val_accuracy:.2f}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-establishment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
